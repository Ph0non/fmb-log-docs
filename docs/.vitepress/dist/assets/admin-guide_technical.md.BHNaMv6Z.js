import{_ as i,c as t,o as n,ag as r}from"./chunks/framework.dvv-DFtf.js";const g=JSON.parse('{"title":"Technische Dokumentation","description":"","frontmatter":{},"headers":[],"relativePath":"admin-guide/technical.md","filePath":"admin-guide/technical.md","lastUpdated":1768196873000}'),o={name:"admin-guide/technical.md"};function l(s,e,a,d,c,u){return n(),t("div",null,[...e[0]||(e[0]=[r('<h1 id="technische-dokumentation" tabindex="-1">Technische Dokumentation <a class="header-anchor" href="#technische-dokumentation" aria-label="Permalink to &quot;Technische Dokumentation&quot;">​</a></h1><p>Dieses Kapitel ergänzt den Admin Guide um technische Details für Betrieb, IT‑Integration und Fehlersuche. Es beschreibt, <strong>wo</strong> FMB Log Daten ablegt, <strong>welche</strong> Dateien „außerhalb der Datenbank“ existieren und <strong>wie</strong> Signaturen/Authentifizierung technisch funktionieren.</p><blockquote><p>Zielgruppe: Admins/Key‑User und IT‑Betreiber (z. B. wenn FMB Log auf mehreren Arbeitsplätzen gegen einen gemeinsamen Hub betrieben wird).</p></blockquote><h2 id="_1-datenablage-resources-vs-hub‐datenordner-vs-applocaldata" tabindex="-1">1) Datenablage: <code>resources</code> vs. Hub‑Datenordner vs. AppLocalData <a class="header-anchor" href="#_1-datenablage-resources-vs-hub‐datenordner-vs-applocaldata" aria-label="Permalink to &quot;1) Datenablage: `resources` vs. Hub‑Datenordner vs. AppLocalData&quot;">​</a></h2><p>FMB Log unterscheidet drei Speicherbereiche:</p><ol><li><strong>Programm‑Ressourcen (<code>resources/</code> im Programmordner)</strong><br> Enthält gebündelte Dateien, die zur App gehören (Extensions, Dictionaries, Logo, Stub‑DB).</li><li><strong>Hub‑Datenordner (gemeinsam, z. B. Netzlaufwerk)</strong><br> Enthält die <strong>Hub‑DB</strong> sowie <strong>Vaults</strong> und <strong>Protokoll‑Archive</strong>. Dieser Ordner ist die „Quelle der Wahrheit“ für den gemeinsamen Betrieb.</li><li><strong>AppLocalData (pro Windows‑Benutzer, lokal)</strong><br> Enthält die <strong>lokale Replica‑DB</strong> und den <strong>lokalen Protokoll‑Cache</strong>, damit die App offline weiter nutzbar bleibt.</li></ol><p>Typische Struktur (vereinfacht):</p><ul><li><strong>Programmordner</strong> (lokal) <ul><li><code>resources/</code><ul><li><code>extensions/crsqlite.dll</code> (CR‑SQLite Extension)</li><li><code>zstd-dicts/rpt-v1.dict</code> (optional, Zstd‑Dictionary für RPT‑Kompression)</li><li><code>Logo_EWN_RGB.png</code> (Logo für Tagesabrechnung)</li><li><code>fmblog.stub.db</code> (Stub‑DB als „Baseline“; wird nur kopiert, wenn noch keine DB existiert)</li></ul></li></ul></li><li><strong>Hub‑Datenordner</strong> (gemeinsam) <ul><li><code>&lt;db&gt;.db</code> (Hub‑DB; Name frei wählbar)</li><li><code>&lt;db&gt;.integrity.pub.json</code> (DB‑Public‑Key)</li><li><code>&lt;db&gt;.integrity.dbkey.json</code> (DB‑Key‑Zertifikat; Root‑signiert)</li><li><code>vaults/</code><ul><li><code>&lt;user_id&gt;.vault</code> (Pepper + User‑Signierschlüssel)</li><li><code>&lt;db&gt;.integrity.vault</code> (DB‑Signierschlüssel; Admin‑Entsperren)</li></ul></li><li><code>protocols/</code><ul><li><code>&lt;site-id-hex&gt;/pack-*.bin</code> (Packfiles für komprimierte Protokolle)</li><li><code>&lt;site-id-hex&gt;/state.json</code> (merkt „aktuelles“ Packfile)</li></ul></li></ul></li><li><strong>AppLocalData</strong> (lokal) <ul><li><code>replicas/fmblog.&lt;hash&gt;.db</code> (Replica‑DB)</li><li><code>protocols-cache/&lt;hub-hash&gt;/</code> (lokaler Protokoll‑Cache; Dateien nach Protokoll‑BLAKE3 benannt)</li></ul></li></ul><div class="info custom-block"><p class="custom-block-title">Zusammenfassung (Speicherorte)</p><ul><li><code>resources/</code> enthält „App‑Ressourcen“ (Extension/Dictionaries/Logo/Stub), nicht die produktiven Daten.</li><li>Produktive Daten liegen im Hub‑Datenordner: Hub‑DB + <code>vaults/</code> + <code>protocols/</code> + Integritätsdateien.</li><li>AppLocalData ist pro Nutzer lokal: Replica‑DB + Cache (offline‑fähig).</li></ul></div><div class="tip custom-block"><p class="custom-block-title">Stub‑DB aktualisieren (bei Schema‑Änderungen)</p><p>Die Stub‑DB ist die „Baseline“ für neue DB‑Dateien (Hub/Replica). Wenn sich das Schema durch neue Migrationen ändert, sollte die Stub‑DB aktualisiert werden:</p><p><code>pnpm -s db:stub:update</code></p><p>Wichtig: Das Skript muss auf <strong>Windows</strong> ausgeführt werden (nicht in WSL), da es die lokal installierten Build‑Tools/Abhängigkeiten nutzt.</p></div><h2 id="_2-messprotokolle-zstd‐kompression-packfiles-und-lokaler-cache" tabindex="-1">2) Messprotokolle: zstd‑Kompression, Packfiles und lokaler Cache <a class="header-anchor" href="#_2-messprotokolle-zstd‐kompression-packfiles-und-lokaler-cache" aria-label="Permalink to &quot;2) Messprotokolle: zstd‑Kompression, Packfiles und lokaler Cache&quot;">​</a></h2><p>Messprotokolle werden aus Performance‑ und Sync‑Gründen <strong>nicht</strong> als BLOB in der CR‑SQLite‑Datenbank synchronisiert. Stattdessen:</p><ol><li>Die Protokoll‑Bytes werden <strong>zstd‑komprimiert</strong> (optional mit Dictionary, z. B. <code>rpt-v1.dict</code>).</li><li>Die komprimierten Bytes werden als <strong>Packfile‑Eintrag</strong> in den Hub geschrieben (append‑only).</li><li>In der DB wird nur eine <strong>Referenz</strong> gespeichert (<code>measurement_protocols</code>): <ul><li><code>pack_file</code>, <code>pack_offset</code>, <code>pack_length</code> (wo liegt das Protokoll im Packfile?)</li><li><code>blake3</code> (Integritäts‑Hash über die komprimierten Bytes; gespeichert als 32‑Byte‑BLOB, angezeigt als Hex)</li><li><code>dict_id</code> (welches Dictionary wurde verwendet, falls vorhanden?)</li></ul></li><li>Zusätzlich wird das Protokoll lokal im <strong>Protokoll‑Cache</strong> abgelegt, damit es offline gelesen werden kann.</li></ol><h3 id="offline‐import-hub-nicht-erreichbar" tabindex="-1">Offline‑Import (Hub nicht erreichbar) <a class="header-anchor" href="#offline‐import-hub-nicht-erreichbar" aria-label="Permalink to &quot;Offline‑Import (Hub nicht erreichbar)&quot;">​</a></h3><p>Offline‑Import ist erlaubt. In diesem Fall:</p><ul><li>Die DB enthält zunächst eine „ausstehende“ Referenz (<code>pack_file=&#39;&#39;</code> und/oder <code>pack_length=0</code>).</li><li>Das Protokoll liegt bereits im lokalen Cache des Import‑Clients (unter dem <code>blake3</code>‑Namen).</li><li>Zusätzlich wird lokal eine Outbox geführt (<code>protocol_upload_outbox</code>), die Upload‑Versuche, Backoff und Fehlerstatus verwaltet.</li><li>Sobald der Hub wieder erreichbar ist, lädt der Import‑Client das Protokoll automatisch in den Hub hoch (Outbox‑Flush) und ergänzt <code>pack_file/offset/length</code>. Der Status ist in der UI auf der Import‑Seite unter „Ausstehende Uploads“ sichtbar und kann dort manuell per „Jetzt synchronisieren“ angestoßen werden.</li></ul><p>Wichtig: Wenn ein Protokoll <strong>noch nicht</strong> im Hub archiviert wurde, können andere Clients die Messung sehen, aber das Protokoll erst öffnen, nachdem der Import‑Client wieder online war und der Upload erfolgt ist.</p><h3 id="cache‐reset" tabindex="-1">Cache‑Reset <a class="header-anchor" href="#cache‐reset" aria-label="Permalink to &quot;Cache‑Reset&quot;">​</a></h3><p>Der lokale Protokoll‑Cache ist ein Performance‑/Offline‑Feature. Ein „Cache leeren“ löscht nur lokale Dateien. Beachten Sie:</p><ul><li>Der Cache ist als <strong>LRU</strong> mit einem Größenlimit ausgelegt (aktuell ~100 MB pro Hub). Bei Bedarf werden ältere, nicht mehr benötigte Einträge automatisch entfernt.</li><li>Wurde ein Protokoll offline importiert und noch nicht in den Hub hochgeladen, darf der Cache <strong>nicht</strong> geleert werden, da sonst der spätere Upload nicht mehr möglich ist. Prüfen Sie vorher „Ausstehende Uploads“ (Import‑Seite): wenn dort ausstehende/pausierte Uploads angezeigt werden, darf der Cache nicht geleert werden. Zusätzlich zeigt die Seite „Cache leeren“ an, ob noch ausstehende Uploads existieren; in diesem Fall ist „Cache leeren“ deaktiviert.</li></ul><h3 id="dictionary‐training-optional" tabindex="-1">Dictionary‑Training (optional) <a class="header-anchor" href="#dictionary‐training-optional" aria-label="Permalink to &quot;Dictionary‑Training (optional)&quot;">​</a></h3><p>Da RPT‑Protokolle oft sehr ähnlich sind, kann ein Zstd‑Dictionary den Speicherbedarf und die I/O‑Last deutlich reduzieren. Das Dictionary kann bei Bedarf neu trainiert werden:</p><ul><li><code>pnpm -s dict:train:rpt</code><br> Trainiert <code>src-tauri/res/zstd-dicts/rpt-v1.dict</code> aus den Beispieldateien unter <code>./RPT</code>.</li></ul><div class="info custom-block"><p class="custom-block-title">Zusammenfassung (Protokolle)</p><ul><li>Protokolle liegen als zstd‑Bytes in <code>protocols/...</code> (Hub) – nicht als DB‑BLOB.</li><li>Integrität wird über <code>measurement_protocols.blake3</code> geprüft (BLAKE3 über komprimierte Bytes).</li><li>Offline‑Import: Protokoll zuerst lokal, Upload in den Hub wird später nachgeholt.</li><li>Cache nur leeren, wenn keine „pending“ Protokolle mehr existieren.</li></ul></div><h2 id="_3-cr‐sqlite-crdt-warum-es-gebraucht-wird-technisch" tabindex="-1">3) CR‑SQLite/CRDT: Warum es gebraucht wird (technisch) <a class="header-anchor" href="#_3-cr‐sqlite-crdt-warum-es-gebraucht-wird-technisch" aria-label="Permalink to &quot;3) CR‑SQLite/CRDT: Warum es gebraucht wird (technisch)&quot;">​</a></h2><p>CR‑SQLite behandelt Tabellen als <strong>CRR</strong> und protokolliert Änderungen in <code>crsql_changes</code>. Daraus ergibt sich:</p><ul><li>Die App kann lokal schreiben (Replica), ohne File‑Locks im Hub „dauerhaft“ zu halten.</li><li>Der Sync ist „best effort“ und führt zu <strong>eventual consistency</strong>: nach erfolgreichem Sync konvergieren alle Replicas deterministisch.</li></ul><p>Technisch wichtig sind Schema‑Constraints von CR‑SQLite:</p><ul><li>Primary Keys müssen non‑NULL sein.</li><li><code>NOT NULL</code>‑Spalten benötigen Defaults (Forward/Backward‑Compatibility).</li><li>Schema‑Änderungen erfolgen über App‑Migrations.</li></ul><p>Die CR‑SQLite Extension wird als Datei <code>resources/extensions/crsqlite.dll</code> gebündelt und vom SQL‑Plugin geladen.</p><div class="info custom-block"><p class="custom-block-title">Zusammenfassung (CR‑SQLite)</p><ul><li>Lokale Replica reduziert Locking‑Probleme auf Netzlaufwerken.</li><li>CR‑SQLite stellt Anforderungen an das Schema → IDs/Defaults sind wichtig.</li><li>Extension liegt als <code>crsqlite.dll</code> im App‑<code>resources</code>‑Ordner.</li></ul></div><h2 id="_4-authentifizierung-passwort-pepper-stronghold-technisch" tabindex="-1">4) Authentifizierung: Passwort + Pepper + Stronghold (technisch) <a class="header-anchor" href="#_4-authentifizierung-passwort-pepper-stronghold-technisch" aria-label="Permalink to &quot;4) Authentifizierung: Passwort + Pepper + Stronghold (technisch)&quot;">​</a></h2><p>FMB Log nutzt zwei Ebenen:</p><ul><li><strong>Passwort‑Hash in der DB:</strong> Argon2id über <code>password + pepper</code> (Pepper ist pro Nutzer zufällig).</li><li><strong>Pepper in Stronghold:</strong> Das Pepper liegt nicht in SQLite, sondern in <code>vaults/&lt;user_id&gt;.vault</code>.</li></ul><p>Login‑Ablauf (vereinfacht):</p><ol><li>Aus eingegebenem Passwort + <code>user_id</code> wird ein <strong>Vault‑Key</strong> abgeleitet (Argon2id, 32‑Byte‑Key).</li><li>Mit diesem Key wird die Stronghold‑Vault entschlüsselt.</li><li>Wenn Entschlüsselung fehlschlägt → Passwort falsch.</li><li>Wenn Entschlüsselung gelingt → Pepper wird gelesen → Hash wird geprüft.</li></ol><div class="info custom-block"><p class="custom-block-title">Zusammenfassung (Auth)</p><ul><li>Hash‑Swap‑Angriffe via DB werden durch user‑spezifisches Pepper verhindert.</li><li>Pepper liegt in Stronghold‑Vault, nicht in SQLite.</li><li>Vault‑Key ist an <code>user_id + password</code> gebunden.</li></ul></div><h2 id="_5-signaturen-delegation-was-wird-womit-signiert" tabindex="-1">5) Signaturen &amp; Delegation: was wird womit signiert? <a class="header-anchor" href="#_5-signaturen-delegation-was-wird-womit-signiert" aria-label="Permalink to &quot;5) Signaturen &amp; Delegation: was wird womit signiert?&quot;">​</a></h2><p>FMB Log signiert verschiedene Datenklassen, um Manipulationen in einer „Datei‑DB“ zu erkennen.</p><h3 id="_5-1-root‐trust-und-db‐signierschlussel-admin" tabindex="-1">5.1 Root‑Trust und DB‑Signierschlüssel (Admin) <a class="header-anchor" href="#_5-1-root‐trust-und-db‐signierschlussel-admin" aria-label="Permalink to &quot;5.1 Root‑Trust und DB‑Signierschlüssel (Admin)&quot;">​</a></h3><ul><li><strong>Root Public Key</strong> ist <strong>fest in der App</strong> eingebaut (Compile‑Time).</li><li>Der <strong>DB‑Public‑Key</strong> liegt als Datei neben der DB: <code>&lt;db&gt;.integrity.pub.json</code>.</li><li>Das <strong>DB‑Key‑Zertifikat</strong> <code>&lt;db&gt;.integrity.dbkey.json</code> ist Root‑signiert und muss vorhanden sein (Fail‑Closed).</li><li>Der <strong>DB‑Private‑Key</strong> liegt in Stronghold: <code>vaults/&lt;db&gt;.integrity.vault</code> und wird per Signier‑Passwort entsperrt.</li></ul><p>Damit werden u. a. signiert:</p><ul><li><code>users</code>, <code>groups</code>, <code>user_groups</code>, <code>group_permissions</code></li><li><code>capability_certs</code> (Delegations‑Zertifikate)</li><li>Admin‑Security‑Einstellungen</li></ul><h3 id="_5-2-user‐signaturen-key‐user-ohne-admin‐signier‐passwort" tabindex="-1">5.2 User‑Signaturen (Key‑User, ohne Admin‑Signier‑Passwort) <a class="header-anchor" href="#_5-2-user‐signaturen-key‐user-ohne-admin‐signier‐passwort" aria-label="Permalink to &quot;5.2 User‑Signaturen (Key‑User, ohne Admin‑Signier‑Passwort)&quot;">​</a></h3><p>Jeder Nutzer besitzt zusätzlich einen <strong>User‑Signierschlüssel</strong> (Ed25519) im eigenen Vault:</p><ul><li><code>vaults/&lt;user_id&gt;.vault</code> enthält neben Pepper auch den User‑Signing‑Key.</li><li>Der Key‑User kann Stammdaten signieren, ohne den DB‑Signier‑Key zu kennen.</li></ul><p>Damit ein Key‑User wirklich „nur in seinem Scope“ signieren kann, wird die Berechtigung über <strong>Capability‑Zertifikate</strong> delegiert:</p><ul><li>Tabelle: <code>capability_certs</code></li><li>Signiert vom DB‑Signier‑Key (Admin‑Aktion)</li><li>Gültigkeit: <code>issued_at</code>, optional <code>expires_at</code>, optional <code>revoked_at</code></li></ul><p>Ein Key‑User darf z. B. Stammdaten nur signieren, wenn ein gültiges Zertifikat für den Scope existiert (z. B. <code>masterdata.fgw</code>, <code>masterdata.nv</code>, <code>masterdata.fmk</code>).</p><div class="info custom-block"><p class="custom-block-title">Zusammenfassung (Signaturen)</p><ul><li>Root‑Trust: Root Public Key ist in der App eingebaut, DB‑Key wird per Zertifikat an Root gebunden.</li><li>Admin‑Key (DB‑Key) signiert Security‑kritische Daten und Delegationen.</li><li>Key‑User signieren Stammdaten per User‑Key + Admin‑Delegation (Capability‑Zertifikat).</li></ul></div><h2 id="_6-hash‐algorithmen-blake3-vs-sha‐256" tabindex="-1">6) Hash‑Algorithmen: BLAKE3 vs. SHA‑256 <a class="header-anchor" href="#_6-hash‐algorithmen-blake3-vs-sha‐256" aria-label="Permalink to &quot;6) Hash‑Algorithmen: BLAKE3 vs. SHA‑256&quot;">​</a></h2><p>In FMB Log werden zwei Hash‑Algorithmen bewusst parallel genutzt:</p><ul><li><strong>BLAKE3</strong> (schnell, parallelisierbar):<br> Für Integrität/Signaturen und Protokoll‑Integrität im Betrieb (Audit, Packfiles, Audit‑Cache).</li><li><strong>SHA‑256</strong> (Standard für externe Systeme):<br> Für RFC3161‑TSA (FreeTSA erwartet SHA‑256) – der Imprint (<code>tsa_snapshot_sha256</code>) und der Token‑Hash (<code>tsa_token_sha256</code>) bleiben SHA‑256.</li></ul><div class="info custom-block"><p class="custom-block-title">Zusammenfassung (Hashes)</p><ul><li>BLAKE3: intern, schnell (Audit/Protokolle/Signatur‑Workflows).</li><li>SHA‑256: extern kompatibel (TSA).</li></ul></div><h2 id="_7-tagesabrechnung-snapshot-qr‐code-tsa-technisch" tabindex="-1">7) Tagesabrechnung: Snapshot, QR‑Code, TSA (technisch) <a class="header-anchor" href="#_7-tagesabrechnung-snapshot-qr‐code-tsa-technisch" aria-label="Permalink to &quot;7) Tagesabrechnung: Snapshot, QR‑Code, TSA (technisch)&quot;">​</a></h2><p>Beim PDF‑Export wird ein <strong>Snapshot</strong> der exportierten Inhalte gebildet und gehasht:</p><ul><li><code>daily_reports.snapshot_hash</code>: BLAKE3 des Snapshots (DATA‑Hash)</li><li><code>daily_reports.pdf_sha256</code>: BLAKE3 der finalen PDF‑Bytes (PDF‑Hash; Spaltenname historisch)</li><li><code>daily_report_invalidations</code>: signierte Invalidierungs‑Events (wer/wann/warum; referenziert Report + Snapshot/PDF‑Hash zum Zeitpunkt der Invalidierung)</li><li>Optional (TSA): <ul><li><code>tsa_provider</code>, <code>tsa_gen_time</code>, <code>tsa_snapshot_sha256</code>, <code>tsa_token_sha256</code>, <code>tsa_token_base64</code></li></ul></li></ul><p>Hinweis: Hashes (32 Byte) und Signaturen (Ed25519, 64 Byte) werden in SQLite als <strong>BLOB</strong> gespeichert (I/O‑sparend). Im Frontend werden BLOBs als <strong>Base64‑Strings</strong> transportiert (keine JSON‑Byte‑Arrays), in der UI werden Hashes aber als <strong>Hex/Fingerprint</strong> angezeigt.</p><p>Die PDF enthält in der Fußzeile:</p><ul><li>einen QR‑Code mit kompakter Payload (u. a. Snapshot‑Hash, optional TSA‑Token‑Hash)</li><li>daneben kurze Fingerprints (<code>DATA: …</code> und optional <code>TSA: …</code>) für manuelle Gegenprüfung</li></ul><div class="info custom-block"><p class="custom-block-title">Zusammenfassung (Tagesabrechnung)</p><ul><li>Snapshot‑Hash ist BLAKE3 (schnell, in QR/Fingerprint als DATA).</li><li>PDF‑Hash ist BLAKE3 (schnell, als Referenz in der Historie).</li><li>Für TSA wird zusätzlich <code>tsa_snapshot_sha256</code> gespeichert (RFC3161‑kompatibler Imprint).</li><li>QR‑Code + Fingerprints ermöglichen schnelle Verifikation in der Historie/Audit.</li></ul></div><h2 id="_8-audit-performance-technisch" tabindex="-1">8) Audit &amp; Performance (technisch) <a class="header-anchor" href="#_8-audit-performance-technisch" aria-label="Permalink to &quot;8) Audit &amp; Performance (technisch)&quot;">​</a></h2><p>Der Audit prüft Security‑Signaturen, Delegationen, Stammdaten‑Signaturen, Messdaten‑Signaturen und Protokoll‑Integrität. Um die Laufzeit zu reduzieren, nutzt FMB Log ein lokales Cache‑Konzept:</p><ul><li>Tabellen: <code>audit_cache</code>, <code>known_keys</code>, <code>known_deps</code></li><li>Enthält BLAKE3‑basierte „Deps/Message/Signature“ Hashes pro Scope/Entity.</li><li><code>audit_cache</code> ist <strong>lokal</strong> (nicht CRR) und wird nicht in den Hub synchronisiert.</li><li>Zusätzlich wird <code>row_db_version</code> gespeichert, damit sich Audits auf geänderte Zeilen beschränken können.</li><li>Um Platz/I/O zu sparen, werden wiederkehrende Hashes (Verify‑Key und Deps) dedupliziert und in <code>audit_cache</code> nur als FK (<code>*_id</code>) referenziert.</li></ul><div class="info custom-block"><p class="custom-block-title">Zusammenfassung (Audit)</p><ul><li>Audit ist vollständig, aber kann teuer sein → Cache + Inkrementalität reduziert Laufzeit.</li><li><code>audit_cache</code> ist pro Client lokal und wird nicht mit dem Hub synchronisiert.</li></ul></div><h2 id="_9-phasenzeiten-performance-log" tabindex="-1">9) Phasenzeiten &amp; Performance-Log <a class="header-anchor" href="#_9-phasenzeiten-performance-log" aria-label="Permalink to &quot;9) Phasenzeiten &amp; Performance-Log&quot;">​</a></h2><p>Bei langen Operationen zeigt FMB Log <strong>Phasenzeiten</strong> im UI an und schreibt dieselben Informationen zusätzlich als strukturierte Einträge (<code>[perf]</code>) in die Diagnose‑Logdatei.</p><p>Betroffene Aktionen:</p><ul><li><strong>Administration → Audit</strong> (prüft Security/Stammdaten/Messdaten/Protokolle)</li><li><strong>Administration → Einstellungen → Stammdaten neu signieren</strong></li><li><strong>Tagesabrechnung → PDF exportieren</strong></li></ul><p>Die Logzeilen enthalten JSON mit <code>action</code>, <code>ok</code>, <code>total_ms</code> und <code>phases</code> (je Phase: <code>name</code>, <code>ms</code>). So können Sie bei Performance‑Problemen nachvollziehen, ob z. B. <strong>I/O (Hub/Protokolle)</strong>, <strong>Hashing</strong> oder <strong>DB‑Zugriffe</strong> dominieren.</p>',70)])])}const p=i(o,[["render",l]]);export{g as __pageData,p as default};
